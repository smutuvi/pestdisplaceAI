{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIYiOXB8aUNp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.util import minibatch, compounding\n",
    "# from seqeval import metrics\n",
    "\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "def convert_doccano_to_spacy(doccano_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines = []\n",
    "        with open(doccano_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['text']\n",
    "            entities = []\n",
    "\n",
    "            for start, end, label in data['labels']:\n",
    "\n",
    "                if label not in ['LOC']:\n",
    "                    entities.append((start, end, label))\n",
    "                # for annotation in data['labels']:\n",
    "                # labels = annotation\n",
    "                # entities.append(labels)\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "        return training_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" +\n",
    "                          doccano_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tar -cvzf healthy.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-06xL42ebP4W"
   },
   "outputs": [],
   "source": [
    "def train_test_data():\n",
    "    filepath = 'data_ciat_spacy/ciat_final.json'\n",
    "    TRAIN_DATA_Converted = convert_doccano_to_spacy(filepath)\n",
    "\n",
    "    TRAINING_DATA = trim_entity_spans(TRAIN_DATA_Converted)\n",
    "\n",
    "    training_data, testing_data = train_test_split(TRAINING_DATA, test_size=0.2, random_state=1)\n",
    "    # print(len(train_data))\n",
    "    # print(len(test_data))\n",
    "    return training_data, testing_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jOzWbdfiblrW"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2F66q-QIhT10"
   },
   "outputs": [],
   "source": [
    "def train_spacy(model, training_data, n_epochs=100):\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "        # create the built-in pipeline components and add them to the pipeline\n",
    "        # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # otherwise, get it so we can add labels\n",
    "    else:\n",
    "        ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in training_data:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        # reset and initialize the weights randomly – but only if we're\n",
    "        # training a new model\n",
    "        if model is None:\n",
    "            nlp.begin_training()\n",
    "        for itn in range(n_epochs):\n",
    "            random.shuffle(training_data)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(training_data, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(\n",
    "                        texts,  # batch of texts\n",
    "                        annotations,  # batch of annotations\n",
    "                        drop=0.5,  # dropout - make it harder to memorise data\n",
    "                        losses=losses,\n",
    "                    )\n",
    "            print(\"epoch: {} Losses: {}\".format(itn, str(losses)))\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8rZIe9OFhXP1",
    "outputId": "9fba1609-d277-4797-e773-65a4d1e27a4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created blank 'en' model\n",
      "epoch: 0 Losses: {'ner': 15462.890112189918}\n",
      "epoch: 1 Losses: {'ner': 2546.6429027457125}\n",
      "epoch: 2 Losses: {'ner': 2086.971809047973}\n",
      "epoch: 3 Losses: {'ner': 1958.0862018113694}\n",
      "epoch: 4 Losses: {'ner': 1713.496927947439}\n",
      "epoch: 5 Losses: {'ner': 1560.2137369937657}\n",
      "epoch: 6 Losses: {'ner': 1372.1907283872551}\n",
      "epoch: 7 Losses: {'ner': 1248.0370129434523}\n",
      "epoch: 8 Losses: {'ner': 1136.9904694945835}\n",
      "epoch: 9 Losses: {'ner': 1126.5190596497246}\n",
      "epoch: 10 Losses: {'ner': 1183.9630931121596}\n",
      "epoch: 11 Losses: {'ner': 1130.966873584017}\n",
      "epoch: 12 Losses: {'ner': 1120.210019589765}\n",
      "epoch: 13 Losses: {'ner': 1009.7791569257392}\n",
      "epoch: 14 Losses: {'ner': 928.9640845634722}\n",
      "epoch: 15 Losses: {'ner': 942.1700517380768}\n",
      "epoch: 16 Losses: {'ner': 916.6272038035324}\n",
      "epoch: 17 Losses: {'ner': 869.6987377356625}\n",
      "epoch: 18 Losses: {'ner': 882.1699191863568}\n",
      "epoch: 19 Losses: {'ner': 765.5053489962635}\n",
      "epoch: 20 Losses: {'ner': 822.5741010019942}\n",
      "epoch: 21 Losses: {'ner': 748.6116617140489}\n",
      "epoch: 22 Losses: {'ner': 732.8768776772486}\n",
      "epoch: 23 Losses: {'ner': 713.1644001101753}\n",
      "epoch: 24 Losses: {'ner': 702.9918243237036}\n",
      "epoch: 25 Losses: {'ner': 699.2492936966216}\n",
      "epoch: 26 Losses: {'ner': 704.1962818779036}\n",
      "epoch: 27 Losses: {'ner': 694.833713815329}\n",
      "epoch: 28 Losses: {'ner': 671.1945995302453}\n",
      "epoch: 29 Losses: {'ner': 729.0991520708117}\n",
      "epoch: 30 Losses: {'ner': 688.9586998404158}\n",
      "epoch: 31 Losses: {'ner': 551.6040986333551}\n",
      "epoch: 32 Losses: {'ner': 574.2661633950679}\n",
      "epoch: 33 Losses: {'ner': 659.0022268016216}\n",
      "epoch: 34 Losses: {'ner': 591.0382291730723}\n",
      "epoch: 35 Losses: {'ner': 565.0727656849728}\n",
      "epoch: 36 Losses: {'ner': 576.0687073554269}\n",
      "epoch: 37 Losses: {'ner': 596.4298013839222}\n",
      "epoch: 38 Losses: {'ner': 601.8600514651715}\n",
      "epoch: 39 Losses: {'ner': 560.3156418371688}\n",
      "epoch: 40 Losses: {'ner': 589.0512149923267}\n",
      "epoch: 41 Losses: {'ner': 520.5157132546822}\n",
      "epoch: 42 Losses: {'ner': 509.06665522805866}\n",
      "epoch: 43 Losses: {'ner': 515.5243214421695}\n",
      "epoch: 44 Losses: {'ner': 461.22006472063174}\n",
      "epoch: 45 Losses: {'ner': 498.9635822677852}\n",
      "epoch: 46 Losses: {'ner': 489.0649472615361}\n",
      "epoch: 47 Losses: {'ner': 507.4350251504282}\n",
      "epoch: 48 Losses: {'ner': 475.24409824495683}\n",
      "epoch: 49 Losses: {'ner': 482.18415989461124}\n",
      "epoch: 50 Losses: {'ner': 530.0048084648528}\n",
      "epoch: 51 Losses: {'ner': 561.1196773233492}\n",
      "epoch: 52 Losses: {'ner': 464.7107946333202}\n",
      "epoch: 53 Losses: {'ner': 449.7816652233671}\n",
      "epoch: 54 Losses: {'ner': 515.7666848300141}\n",
      "epoch: 55 Losses: {'ner': 466.0702509490723}\n",
      "epoch: 56 Losses: {'ner': 420.92735105766496}\n",
      "epoch: 57 Losses: {'ner': 445.76000062256384}\n",
      "epoch: 58 Losses: {'ner': 410.75412332713194}\n",
      "epoch: 59 Losses: {'ner': 491.1493344505199}\n",
      "epoch: 60 Losses: {'ner': 408.98859937770186}\n",
      "epoch: 61 Losses: {'ner': 406.51595269125914}\n",
      "epoch: 62 Losses: {'ner': 426.1253625416894}\n",
      "epoch: 63 Losses: {'ner': 429.81882291971215}\n",
      "epoch: 64 Losses: {'ner': 432.0524537738653}\n",
      "epoch: 65 Losses: {'ner': 470.69533166768485}\n",
      "epoch: 66 Losses: {'ner': 410.8351338763814}\n",
      "epoch: 67 Losses: {'ner': 447.9806123290764}\n",
      "epoch: 68 Losses: {'ner': 397.4338047657068}\n",
      "epoch: 69 Losses: {'ner': 396.462809123873}\n",
      "epoch: 70 Losses: {'ner': 360.2740703365977}\n",
      "epoch: 71 Losses: {'ner': 335.2461513029883}\n",
      "epoch: 72 Losses: {'ner': 361.63799742747443}\n",
      "epoch: 73 Losses: {'ner': 371.9495263576821}\n",
      "epoch: 74 Losses: {'ner': 415.5196807173263}\n",
      "epoch: 75 Losses: {'ner': 376.78908234210957}\n",
      "epoch: 76 Losses: {'ner': 396.38200053001106}\n",
      "epoch: 77 Losses: {'ner': 343.6448299568135}\n",
      "epoch: 78 Losses: {'ner': 361.3119677040495}\n",
      "epoch: 79 Losses: {'ner': 376.3547024643011}\n",
      "epoch: 80 Losses: {'ner': 324.41195052864055}\n",
      "epoch: 81 Losses: {'ner': 329.4672598185481}\n",
      "epoch: 82 Losses: {'ner': 313.7711690672546}\n",
      "epoch: 83 Losses: {'ner': 455.3768569535745}\n",
      "epoch: 84 Losses: {'ner': 346.3366792922321}\n",
      "epoch: 85 Losses: {'ner': 341.7086104674896}\n",
      "epoch: 86 Losses: {'ner': 328.08284596585395}\n",
      "epoch: 87 Losses: {'ner': 291.59011411860394}\n",
      "epoch: 88 Losses: {'ner': 295.0027656852484}\n",
      "epoch: 89 Losses: {'ner': 307.0675036307192}\n",
      "epoch: 90 Losses: {'ner': 279.9497504834194}\n",
      "epoch: 91 Losses: {'ner': 296.58797361944175}\n",
      "epoch: 92 Losses: {'ner': 294.6501281950851}\n",
      "epoch: 93 Losses: {'ner': 370.5680468782413}\n",
      "epoch: 94 Losses: {'ner': 335.2478879771331}\n",
      "epoch: 95 Losses: {'ner': 304.41702190653774}\n",
      "epoch: 96 Losses: {'ner': 297.055871793415}\n",
      "epoch: 97 Losses: {'ner': 287.4249382115309}\n",
      "epoch: 98 Losses: {'ner': 286.64836285176585}\n",
      "epoch: 99 Losses: {'ner': 245.43655682993148}\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_spacy(None, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rHf3Aq-nsz6S"
   },
   "source": [
    "###Training approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XlYhRX30cW-7",
    "outputId": "e7495632-d826-442c-aa7f-4fb80a169217"
   },
   "outputs": [],
   "source": [
    "# new_labels = set()\n",
    "# for entry in train_data:\n",
    "#     for label in entry[1][\"entities\"]:\n",
    "#         new_labels.add(label[2])\n",
    "# new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wzcnnIzWcDrZ"
   },
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import random\n",
    "\n",
    "# nlp = spacy.blank('en')\n",
    "# ner = nlp.create_pipe('ner')\n",
    "# nlp.add_pipe(ner)\n",
    "\n",
    "\n",
    "# for label in new_labels:\n",
    "#     ner.add_label(label)   # add new entity label to entity recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eeBa15VMgSLO"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# from spacy.util import minibatch\n",
    "\n",
    "# optimizer = nlp.begin_training()\n",
    "# n_iter = 100\n",
    "# for itn in range(n_iter):\n",
    "#     # i = 0\n",
    "#     random.shuffle(train_data)\n",
    "#     losses = {}\n",
    "#     batches = minibatch(train_data, 100)\n",
    "#     for batch in batches:\n",
    "#         texts, annotations = zip(*batch)\n",
    "#         nlp.update(texts, annotations, sgd=optimizer, drop=0.5, losses=losses)\n",
    "#         # if i%100 == 0: \n",
    "#         #     print(losses)\n",
    "#         # i += 1\n",
    "#     print(\"Loss after epoch\", itn, \"=\", losses[\"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P68URJSYtFwK"
   },
   "source": [
    "###End of training approach- 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "iD8mI3PFgrNs",
    "outputId": "33e513b5-c28f-4b36-e001-c60c9908caef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uas': 0.0, 'las': 0.0, 'las_per_type': {'': {'p': 0.0, 'r': 0.0, 'f': 0.0}}, 'ents_p': 78.18181818181819, 'ents_r': 63.23529411764706, 'ents_f': 69.91869918699187, 'ents_per_type': {'Disease': {'p': 60.0, 'r': 54.54545454545454, 'f': 57.14285714285713}, 'GPE': {'p': 88.57142857142857, 'r': 67.3913043478261, 'f': 76.5432098765432}}, 'tags_acc': 0.0, 'token_acc': 100.0, 'textcat_score': 0.0, 'textcats_per_cat': {}}\n"
     ]
    }
   ],
   "source": [
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "nlp = trained_model\n",
    "scorer = Scorer()\n",
    "for input_, annot in test_data:\n",
    "    doc_gold_text = nlp.make_doc(input_)\n",
    "    gold = GoldParse(doc_gold_text, entities=annot[\"entities\"])\n",
    "    pred_value = nlp(input_)\n",
    "    scorer.score(pred_value, gold)\n",
    "print(scorer.scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-CgsjGkr5kp"
   },
   "outputs": [],
   "source": [
    "# !rm -R out/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e_QuHAgckJaj",
    "outputId": "268ff0e7-188d-4989-eb4b-953b4ed16a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to out_ciat\n"
     ]
    }
   ],
   "source": [
    "# save model to output directory\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "dt = datetime.now().strftime(\"%d-%m-%Y.%H-%M\")\n",
    "\n",
    "\n",
    "output_dir = Path(\"out_ciat\")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp.meta['name'] = \"NER \" + dt # rename model\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TBTbLOy5m5CD",
    "outputId": "e89bc0fd-0a46-4139-b9e1-4af2368ddf9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "gl2c59oXkX6G",
    "outputId": "ab6e46fa-f843-40b4-c495-29b069d804f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disease Potato mop-top\n",
      "Disease Potato mop-top\n",
      "GPE Spongospora\n",
      "GPE Illumina\n",
      "GPE Chiloé Province\n",
      "GPE the United States\n",
      "GPE near Chiloé Province\n",
      "[16, 30, 'Disease']\n",
      "[40, 45, 'GPE']\n",
      "[48, 62, 'Disease']\n",
      "[625, 630, 'GPE']\n",
      "[1579, 1594, 'GPE']\n",
      "[2741, 2746, 'GPE']\n",
      "[2869, 2884, 'GPE']\n",
      "[3050, 3055, 'GPE']\n",
      "[599, 614, 'GPE']\n",
      "[579, 585, 'GPE']\n",
      "[1142, 1148, 'GPE']\n"
     ]
    }
   ],
   "source": [
    "nlp2 = spacy.load(\"out_ciat\")\n",
    "doc2 = nlp2(test_data[2][0])\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)\n",
    "for e in test_data[2][1][\"entities\"]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: out_ciat/ (stored 0%)\n",
      "  adding: out_ciat/vocab/ (stored 0%)\n",
      "  adding: out_ciat/vocab/strings.json (deflated 69%)\n",
      "  adding: out_ciat/vocab/vectors (deflated 45%)\n",
      "  adding: out_ciat/vocab/key2row (stored 0%)\n",
      "  adding: out_ciat/vocab/lexemes.bin (deflated 73%)\n",
      "  adding: out_ciat/ner/ (stored 0%)\n",
      "  adding: out_ciat/ner/cfg (deflated 47%)\n",
      "  adding: out_ciat/ner/model (deflated 7%)\n",
      "  adding: out_ciat/ner/moves (deflated 56%)\n",
      "  adding: out_ciat/meta.json (deflated 37%)\n",
      "  adding: out_ciat/tokenizer (deflated 82%)\n"
     ]
    }
   ],
   "source": [
    "# !zip -r out_ciat.zip out_ciat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ciat_ner.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
